{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12c3df4-116b-4568-8ba1-1f7919ff869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by importing all the required libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33bdeb0-d70b-4fd9-b205-972ce8774f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to initialize two parameters for each of the neurons in each layer: 1) Weight and 2) Bias.\n",
    "def init_params(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return params\n",
    "\n",
    "# Z (linear hypothesis) - Z = W*X + b , \n",
    "# W - weight matrix, b- bias vector, X- Input \n",
    "\n",
    "# Creamos la funcion de activacion sigmoide \n",
    "def sigmoid(Z):\n",
    "    # Aplicamos la funcion sigmoide a Z hipotesis lineal\n",
    "    A = 1/(1+np.exp(np.dot(-1, Z)))\n",
    "    cache = (Z) # Guardamos Z en la cache para usarla en la implementacion de backpropagation\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2e9cb-dfd9-406f-b4ec-76167a27720a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
